<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on BFNet Docs</title>
    <link>//localhost:1313/docs/network/kubernetes/</link>
    <description>Recent content in Kubernetes on BFNet Docs</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Sep 2023 15:40:51 -0400</lastBuildDate>
    <atom:link href="//localhost:1313/docs/network/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>K3s Rebuild</title>
      <link>//localhost:1313/docs/network/kubernetes/k3srebuild/</link>
      <pubDate>Thu, 21 Sep 2023 15:40:51 -0400</pubDate>
      <guid>//localhost:1313/docs/network/kubernetes/k3srebuild/</guid>
      <description>Remove current cluster link On worker node: sudo /var/local/bin/k3s-agent-uninstall.sh sudo /var/local/bin/k3s-killall.sh sudo rm -R /var/lib/rancher/k3s/agent sudo rm -R /var/lib/rancher/k3s/data sudo rm -R /var/lib/rancher/k3s/server On the master: sudo /var/local/bin/k3s-uninstall.sh sudo /var/local/bin/k3s-killall.sh sudo rm -R /var/lib/rancher/k3s/agent sudo rm -R /var/lib/rancher/k3s/data sudo rm -R /var/lib/rancher/k3s/server &amp;hellip;or image a new K3os instance link Burn Armbian image to ssd card Shutdown cluster and remove all modules Image sopine0 install module0 boot &amp;hellip;try link Armbian_20.11_Pine64so_bionic_current_5.8.16 No ethernet Armbian_20.</description>
    </item>
    <item>
      <title>Kubernete Cluster</title>
      <link>//localhost:1313/docs/network/kubernetes/kubernetecluster/</link>
      <pubDate>Thu, 21 Sep 2023 15:39:22 -0400</pubDate>
      <guid>//localhost:1313/docs/network/kubernetes/kubernetecluster/</guid>
      <description>Rancher linkA cluster was added to the host pinarello, and Rancher was installed. Trying to add the felt cluster fails with a CrashLoopBackOff status. The log of the pod shows:&#xA;sbrown@bianchi ~]$ kubectl logs -n cattle-system cattle-cluster-agent-6bb97bd9ff-qbdk2 INFO: Environment: CATTLE_ADDRESS=10.42.0.9 CATTLE_CA_CHECKSUM=0df5882f1714e8d8989215b7c25b59fd2cd623f62d568dd8adc34d1729c6fb06 CATTLE_CLUSTER=true CATTLE_CLUSTER_AGENT_PORT=tcp://10.43.129.180:80 CATTLE_CLUSTER_AGENT_PORT_443_TCP=tcp://10.43.129.180:443 CATTLE_CLUSTER_AGENT_PORT_443_TCP_ADDR=10.43.129.180 CATTLE_CLUSTER_AGENT_PORT_443_TCP_PORT=443 CATTLE_CLUSTER_AGENT_PORT_443_TCP_PROTO=tcp CATTLE_CLUSTER_AGENT_PORT_80_TCP=tcp://10.43.129.180:80 CATTLE_CLUSTER_AGENT_PORT_80_TCP_ADDR=10.43.129.180 CATTLE_CLUSTER_AGENT_PORT_80_TCP_PORT=80 CATTLE_CLUSTER_AGENT_PORT_80_TCP_PROTO=tcp CATTLE_CLUSTER_AGENT_SERVICE_HOST=10.43.129.180 CATTLE_CLUSTER_AGENT_SERVICE_PORT=80 CATTLE_CLUSTER_AGENT_SERVICE_PORT_HTTP=80 CATTLE_CLUSTER_AGENT_SERVICE_PORT_HTTPS_INTERNAL=443 CATTLE_CLUSTER_REGISTRY= CATTLE_INGRESS_IP_DOMAIN=sslip.io CATTLE_INSTALL_UUID=2ce92d8c-5a0a-485e-83cd-910a362cf557 CATTLE_INTERNAL_ADDRESS= CATTLE_IS_RKE=false CATTLE_K8S_MANAGED=true CATTLE_NODE_NAME=cattle-cluster-agent-6bb97bd9ff-qbdk2 CATTLE_SERVER=https://pinarello.6browns.org CATTLE_SERVER_VERSION=v2.6.6 INFO: Using resolv.conf: search cattle-system.svc.cluster.local svc.cluster.local cluster.</description>
    </item>
    <item>
      <title>Cloudflare and Kubernetes</title>
      <link>//localhost:1313/docs/network/kubernetes/cloudflarekubernetes/</link>
      <pubDate>Thu, 21 Sep 2023 15:19:46 -0400</pubDate>
      <guid>//localhost:1313/docs/network/kubernetes/cloudflarekubernetes/</guid>
      <description>Cloudflared tunnel on K3s, connect to ingress, instead of service. That would allow trusted certificates from both internal sources and the CF system.&#xA;One post says, Cloudflare tunnel can only link to a service. In that case the ingress can be made for the internal service, and some other means for the tunnel. At the moment the noTLScheck option is set.&#xA;https://blog.cloudflare.com/automated-origin-ca-for-kubernetes/&#xA;https://blog.cloudflare.com/cloudflare-ca-encryption-origin/&#xA;K3s cloudflared tunnel, check &amp;ldquo;access&amp;rdquo; option.&#xA;Gitea linkThe instance is running using the included DB.</description>
    </item>
    <item>
      <title>CoreDNS Troubleshooting</title>
      <link>//localhost:1313/docs/network/kubernetes/corednstroubleshooting/</link>
      <pubDate>Thu, 21 Sep 2023 14:28:17 -0400</pubDate>
      <guid>//localhost:1313/docs/network/kubernetes/corednstroubleshooting/</guid>
      <description>Test Cluster linkRun a quick Alpine instance for debugging:&#xA;kubectl run -it --rm --restart=Never alpine --image=alpine sh Ping gateway on another node 3: eth0@if21: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1450 qdisc noqueue state UP link/ether aa:82:29:16:75:a2 brd ff:ff:ff:ff:ff:ff inet 10.42.1.3/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a882:29ff:fe16:75a2/64 scope link valid_lft forever preferred_lft forever ping 10.42.2.1 PING 10.42.2.1 (10.42.2.1): 56 data bytes 64 bytes from 10.42.2.1: seq=0 ttl=63 time=55.057 ms Lookup external host name / # nslookup slashdot.</description>
    </item>
  </channel>
</rss>
